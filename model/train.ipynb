{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data.csv\"\n",
    "data_raw = pd.read_csv(data_path)\n",
    "print(\"Number of rows in data =\",data_raw.shape[0])\n",
    "print(\"Number of columns in data =\",data_raw.shape[1])\n",
    "print(\"\\n\")\n",
    "print(\"**Sample data:**\")\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(data_raw.columns.values)[3:12]\n",
    "print(categories)\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(20,30))\n",
    "print(data_raw.iloc[:,3:12].sum())\n",
    "ax= sns.barplot(categories, data_raw.iloc[:,3:12].sum().values)\n",
    "plt.title(\"Review in each category\", fontsize=24)\n",
    "plt.ylabel('Number of Review', fontsize=18)\n",
    "plt.xlabel('Experiance Type ', fontsize=18)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = data_raw.iloc[:,3:].sum().values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowSums = data_raw.iloc[:,2:].sum(axis=1)\n",
    "multiLabel_counts = rowSums.value_counts()\n",
    "multiLabel_counts = multiLabel_counts.iloc[:]\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(40,10))\n",
    "ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n",
    "plt.title(\"Reviews having multiple labels \")\n",
    "plt.ylabel('Number of reviews', fontsize=18)\n",
    "plt.xlabel('Number of labels', fontsize=18)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = multiLabel_counts.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Pre-Processing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "data = data_raw\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "data['Review'] = data['Review'].str.lower()\n",
    "data['Review'] = data['Review'].apply(cleanHtml)\n",
    "data['Review'] = data['Review'].apply(cleanPunc)\n",
    "data['Review'] = data['Review'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "data['Review'] = data['Review'].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "data['Review'] = data['Review'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and train data partitioning...\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, random_state=42, test_size=0.10, shuffle=True)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['Review']\n",
    "test_text = test['Review']\n",
    "#print(\"trian\")\n",
    "#print(train)\n",
    "#print(\"test\")\n",
    "#print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)\n",
    "tfidf_vectorizer_file = open(\"tf_idf_vectorizer.pickle\",\"wb\")\n",
    "pickle.dump(vectorizer, tfidf_vectorizer_file)\n",
    "tfidf_vectorizer_file.close()\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "#print(\"train_text\")\n",
    "#print(train_text)\n",
    "#print(\"x_train\")\n",
    "#print(x_train)\n",
    "y_train = train.drop(labels = ['Review'], axis=1)\n",
    "print(y_train)\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels = ['Review'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Binary Classifications - (One Vs Rest Classifier)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from IPython.display import Markdown, display\n",
    "import pickle\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "%time\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "arrs = []\n",
    "#print(str(pickle.dumps(LogReg_pipeline)))\n",
    "#print(type(LogReg_pipeline))\n",
    "pipeline_array = []\n",
    "for category in categories:\n",
    "    printmd('**Processing {} review...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    #print(\"x_train\")\n",
    "    #print(x_train)\n",
    "    LogReg_pipeline.fit(x_train, train[category])\n",
    "    our_model = pickle.dumps(LogReg_pipeline)\n",
    "    pipeline_array.append(our_model)\n",
    "    #print(train[category])\n",
    "    # calculating test accuracy\n",
    "    #print(\"x_test\")\n",
    "    #print(x_test)\n",
    "    #prediction = LogReg_pipeline.predict(x_test)\n",
    "    #arrs.append(prediction)\n",
    "    #print(\"Prediction: \")\n",
    "    #print(prediction)\n",
    "    #print('Test accuracy is {}'.format(accuracy_score(test[category], prediction)))\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    \n",
    "pickle_out = open(\"trained_model.pickle\",\"wb\")\n",
    "pickle.dump(pipeline_array, pickle_out)\n",
    "pickle_out.close()\n",
    "  \n",
    "print(\"Trained model saved in the file \"+pickle_out.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
